{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "data = pd.read_csv('../data/df_charac.csv')\n",
    "   \n",
    "data = data.dropna()\n",
    "\n",
    "def iqr(df, columns):\n",
    "    Q1 = df[columns].quantile(0.25)\n",
    "    Q3 = df[columns].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    \n",
    "    df_clipped = df.copy()\n",
    "    \n",
    "    for column in columns:\n",
    "        df_clipped[column] = df[column].clip(lower=lower[column], upper=upper[column])\n",
    "        \n",
    "    return df_clipped\n",
    "\n",
    "data = iqr(data, ['visitors', 'visit/cost'])\n",
    "\n",
    "data = pd.get_dummies(data, columns=['target'], drop_first=False)\n",
    "\n",
    "X = data[['month', \n",
    "          'cost', \n",
    "          'target_family', \n",
    "          'target_old', \n",
    "          'target_youth',\n",
    "          'Fe_festival_conc',\n",
    "          'non_festival_conc',\n",
    "          'non_local',\n",
    "          'non_foreigner'\n",
    "          ]]\n",
    "y_visitors = data['visitors']\n",
    "y_vicost = data['visit/cost'] \n",
    "X_train, X_test, y_train_visitors, y_test_visitors, y_train_vicost, y_test_vicost = train_test_split(\n",
    "X, y_visitors, y_vicost, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_models(best_visitors, best_vicost, X_test, y_test_visitors, y_test_vicost):\n",
    "    # Visitors 모델 평가\n",
    "    y_pred_visitors = best_visitors.predict(X_test)\n",
    "    mse_visitors = mean_squared_error(y_test_visitors, y_pred_visitors)\n",
    "    r2_visitors = r2_score(y_test_visitors, y_pred_visitors)\n",
    "\n",
    "    # Visit/Cost 모델 평가\n",
    "    y_pred_vicost = best_vicost.predict(X_test)\n",
    "    mse_vicost = mean_squared_error(y_test_vicost, y_pred_vicost)\n",
    "    r2_vicost = r2_score(y_test_vicost, y_pred_vicost)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"Visitors Model Performance:\")\n",
    "    print(f\"  MSE: {mse_visitors}\")\n",
    "    print(f\"  R²: {r2_visitors}\")\n",
    "\n",
    "    print(f\"Visit/Cost Model Performance:\")\n",
    "    print(f\"  MSE: {mse_vicost}\")\n",
    "    print(f\"  R²: {r2_vicost}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def train_models(X_train, y_train_visitors, y_train_vicost):\n",
    "    rf_visitors = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf_visitors.fit(X_train, y_train_visitors)\n",
    "\n",
    "    rf_vicost = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf_vicost.fit(X_train, y_train_vicost)\n",
    "    \n",
    "    return rf_visitors, rf_vicost\n",
    "\n",
    "rf_visitors, rf_vicost = train_models(X_train, y_train_visitors, y_train_vicost)\n",
    "\n",
    "evaluate_models(rf_visitors, rf_vicost, X_test, y_test_visitors, y_test_vicost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best visitors RF : {'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 70}\n",
      "Best visit/cost RF : {'max_depth': 3, 'min_samples_split': 7, 'n_estimators': 70}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def tune_rf(X_train, y_train_visitors, y_train_vicost):\n",
    "    param_grid_rf = {\n",
    "        'n_estimators': np.arange(50, 90, 20),\n",
    "        'max_depth': [3,15,17],\n",
    "        'min_samples_split':  [2, 7,8]}\n",
    "\n",
    "    rf_visitors = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    grid_search_rf_visitors = GridSearchCV(estimator=rf_visitors, param_grid=param_grid_rf, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search_rf_visitors.fit(X_train, y_train_visitors)\n",
    "    \n",
    "    rf_vicost = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    grid_search_rf_vicost = GridSearchCV(estimator=rf_vicost, param_grid=param_grid_rf, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search_rf_vicost.fit(X_train, y_train_vicost)\n",
    "    \n",
    "    print(f\"Best visitors RF : {grid_search_rf_visitors.best_params_}\")\n",
    "    print(f\"Best visit/cost RF : {grid_search_rf_vicost.best_params_}\")\n",
    "\n",
    "    return grid_search_rf_visitors.best_estimator_, grid_search_rf_vicost.best_estimator_\n",
    "\n",
    "best_rf_visitors, best_rf_vicost = tune_rf(X_train, y_train_visitors, y_train_vicost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visitors Model Performance:\n",
      "  MSE: 5610478564.934263\n",
      "  R²: 0.40267433485593784\n",
      "Visit/Cost Model Performance:\n",
      "  MSE: 8.18599080688758e-09\n",
      "  R²: 0.6215148515716399\n"
     ]
    }
   ],
   "source": [
    "evaluate_models(best_rf_visitors, best_rf_vicost, X_test, y_test_visitors, y_test_vicost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def train_xgboost(X_train, y_train_visitors, y_train_vicost):\n",
    "    \n",
    "    xgb_visitors = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "    xgb_visitors.fit(X_train, y_train_visitors)\n",
    "\n",
    "    xgb_vicost = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "    xgb_vicost.fit(X_train, y_train_vicost)\n",
    "    \n",
    "    return xgb_visitors, xgb_vicost\n",
    "\n",
    "xgb_visitors, xgb_vicost = train_xgboost(X_train, y_train_visitors, y_train_vicost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest xgb visit/cost: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search_xgb_vicost\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grid_search_xgb_visitors\u001b[38;5;241m.\u001b[39mbest_estimator_, grid_search_xgb_vicost\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m---> 22\u001b[0m best_xgb_visitors, best_xgb_vicost \u001b[38;5;241m=\u001b[39m tune_xgb(X_train, y_train_visitors, y_train_vicost)\n",
      "Cell \u001b[1;32mIn[30], line 11\u001b[0m, in \u001b[0;36mtune_xgb\u001b[1;34m(X_train, y_train_visitors, y_train_vicost)\u001b[0m\n\u001b[0;32m      9\u001b[0m xgb_visitors \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m grid_search_xgb_visitors \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mxgb_visitors, param_grid\u001b[38;5;241m=\u001b[39mparam_grid_xgb, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m grid_search_xgb_visitors\u001b[38;5;241m.\u001b[39mfit(X_train, y_train_visitors)\n\u001b[0;32m     13\u001b[0m xgb_vicost \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m grid_search_xgb_vicost \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mxgb_vicost, param_grid\u001b[38;5;241m=\u001b[39mparam_grid_xgb, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1572\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     )\n\u001b[1;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    965\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    966\u001b[0m         clone(base_estimator),\n\u001b[0;32m    967\u001b[0m         X,\n\u001b[0;32m    968\u001b[0m         y,\n\u001b[0;32m    969\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    970\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    971\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    972\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    973\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    974\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    975\u001b[0m     )\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    978\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    979\u001b[0m     )\n\u001b[0;32m    980\u001b[0m )\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m     )\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1108\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1105\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m-> 1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m   1109\u001b[0m     params,\n\u001b[0;32m   1110\u001b[0m     train_dmatrix,\n\u001b[0;32m   1111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_boosting_rounds(),\n\u001b[0;32m   1112\u001b[0m     evals\u001b[38;5;241m=\u001b[39mevals,\n\u001b[0;32m   1113\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_rounds,\n\u001b[0;32m   1114\u001b[0m     evals_result\u001b[38;5;241m=\u001b[39mevals_result,\n\u001b[0;32m   1115\u001b[0m     obj\u001b[38;5;241m=\u001b[39mobj,\n\u001b[0;32m   1116\u001b[0m     custom_metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[0;32m   1117\u001b[0m     verbose_eval\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   1118\u001b[0m     xgb_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1119\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, iteration\u001b[38;5;241m=\u001b[39mi, fobj\u001b[38;5;241m=\u001b[39mobj)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2100\u001b[0m     _check_call(\n\u001b[1;32m-> 2101\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[0;32m   2102\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, ctypes\u001b[38;5;241m.\u001b[39mc_int(iteration), dtrain\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   2103\u001b[0m         )\n\u001b[0;32m   2104\u001b[0m     )\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def tune_xgb(X_train, y_train_visitors, y_train_vicost):\n",
    "    param_grid_xgb = {\n",
    "        'n_estimators': np.arange(90,110,10),\n",
    "        'max_depth': [10, 11, 12],\n",
    "        'learning_rate': np.arange(0.02,0.04,0.01),\n",
    "        'subsample': [0.65,0.7,0.75],\n",
    "        'colsample_bytree': np.arange(0.6, 0.8, 0.1)}\n",
    "\n",
    "    xgb_visitors = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
    "    grid_search_xgb_visitors = GridSearchCV(estimator=xgb_visitors, param_grid=param_grid_xgb, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search_xgb_visitors.fit(X_train, y_train_visitors)\n",
    "\n",
    "    xgb_vicost = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
    "    grid_search_xgb_vicost = GridSearchCV(estimator=xgb_vicost, param_grid=param_grid_xgb, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search_xgb_vicost.fit(X_train, y_train_vicost)\n",
    "    \n",
    "    print(f\"Best xgb visitors: {grid_search_xgb_visitors.best_params_}\")\n",
    "    print(f\"Best xgb visit/cost: {grid_search_xgb_vicost.best_params_}\")\n",
    "\n",
    "    return grid_search_xgb_visitors.best_estimator_, grid_search_xgb_vicost.best_estimator_\n",
    "\n",
    "best_xgb_visitors, best_xgb_vicost = tune_xgb(X_train, y_train_visitors, y_train_vicost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visitors Model Performance:\n",
      "  MSE: 5590753632.578279\n",
      "  R²: 0.40477437110117\n",
      "Visit/Cost Model Performance:\n",
      "  MSE: 1.593508656767585e-08\n",
      "  R²: 0.26322985853942094\n"
     ]
    }
   ],
   "source": [
    "evaluate_models(best_xgb_visitors, best_xgb_vicost, X_test, y_test_visitors, y_test_vicost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visitors Model Performance:\n",
      "  MSE: 7502673106.285818\n",
      "  R²: 0.2012197976157578\n",
      "Visit/Cost Model Performance:\n",
      "  MSE: 9.136792145909058e-09\n",
      "  R²: 0.5775538706207888\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def train_gradient_boosting(X_train, y_train_visitors, y_train_vicost):\n",
    "    gb_visitors = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "    gb_visitors.fit(X_train, y_train_visitors)\n",
    "\n",
    "    gb_vicost = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "    gb_vicost.fit(X_train, y_train_vicost)\n",
    "    \n",
    "    return gb_visitors, gb_vicost\n",
    "\n",
    "gb_visitors, gb_vicost = train_gradient_boosting(X_train, y_train_visitors, y_train_vicost)\n",
    "\n",
    "evaluate_models(gb_visitors, gb_vicost, X_test, y_test_visitors, y_test_vicost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gb visitors: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 25, 'subsample': 1.0}\n",
      "Best gb visit/cost: {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 50, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def tune_gradient_boosting(X_train, y_train_visitors, y_train_vicost):\n",
    "    param_grid_gb = {\n",
    "        'n_estimators': [25, 50, 150, 200],\n",
    "        'max_depth': [2, 3, 4],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'subsample': [0.5, 0.8, 1.0]}\n",
    "    \n",
    "    gb_visitors = GradientBoostingRegressor(random_state=42)\n",
    "    grid_search_gb_visitors = GridSearchCV(estimator=gb_visitors, param_grid=param_grid_gb, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search_gb_visitors.fit(X_train, y_train_visitors)\n",
    "\n",
    "    gb_vicost = GradientBoostingRegressor(random_state=42)\n",
    "    grid_search_gb_vicost = GridSearchCV(estimator=gb_vicost, param_grid=param_grid_gb, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search_gb_vicost.fit(X_train, y_train_vicost)\n",
    "\n",
    "    print(f\"Best gb visitors: {grid_search_gb_visitors.best_params_}\")\n",
    "    print(f\"Best gb visit/cost: {grid_search_gb_vicost.best_params_}\")\n",
    "\n",
    "    return grid_search_gb_visitors.best_estimator_, grid_search_gb_vicost.best_estimator_\n",
    "\n",
    "best_gb_visitors, best_gb_vicost = tune_gradient_boosting(X_train, y_train_visitors, y_train_vicost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visitors Model Performance:\n",
      "  MSE: 7609133369.842461\n",
      "  R²: 0.18988539057643083\n",
      "Visit/Cost Model Performance:\n",
      "  MSE: 8.812427945560647e-09\n",
      "  R²: 0.5925510817598997\n"
     ]
    }
   ],
   "source": [
    "evaluate_models(best_gb_visitors, best_gb_vicost, X_test, y_test_visitors, y_test_vicost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visitors Model Performance:\n",
      "  MSE: 8816689686.666874\n",
      "  R²: 0.06132160198017089\n",
      "Visit/Cost Model Performance:\n",
      "  MSE: 2.43198254509604e-08\n",
      "  R²: -0.12444454956067807\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RANSACRegressor\n",
    "\n",
    "def train_ransac(X_train, y_train_visitors, y_train_vicost):\n",
    "    ransac_visitors = RANSACRegressor(random_state=42)\n",
    "    ransac_visitors.fit(X_train, y_train_visitors)\n",
    "\n",
    "    ransac_vicost = RANSACRegressor(random_state=42)\n",
    "    ransac_vicost.fit(X_train, y_train_vicost)\n",
    "    \n",
    "    return ransac_visitors, ransac_vicost\n",
    "\n",
    "ransac_visitors, ransac_vicost = train_ransac(X_train, y_train_visitors, y_train_vicost)\n",
    "\n",
    "evaluate_models(ransac_visitors, ransac_vicost, X_test, y_test_visitors, y_test_vicost)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gb visitors: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 25, 'subsample': 1.0}\n",
      "Best gb visit/cost: {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 50, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tune_ransac(X_train, y_train_visitors, y_train_vicost):\n",
    "    param_grid_ransac = {\n",
    "        'min_samples': [0.5, 0.7, 0.9],\n",
    "        'residual_threshold': [5, 10, 20],\n",
    "        'max_trials': [50, 100, 150],\n",
    "    }\n",
    "\n",
    "    ransac_visitors = RANSACRegressor(random_state=42, n_jobs=-1)\n",
    "    grid_search_ransac_visitors = GridSearchCV(estimator=ransac_visitors, param_grid=param_grid_ransac, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search_ransac_visitors.fit(X_train, y_train_visitors)\n",
    "\n",
    "    ransac_vicost = RANSACRegressor(random_state=42, n_jobs=-1)\n",
    "    grid_search_ransac_vicost = GridSearchCV(estimator=ransac_vicost, param_grid=param_grid_ransac, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search_ransac_vicost.fit(X_train, y_train_vicost)\n",
    "\n",
    "    print(f\"Best rs visitors: {grid_search_ransac_visitors.best_params_}\")\n",
    "    print(f\"Best rs visit/cost: {grid_search_ransac_vicost.best_params_}\")\n",
    "\n",
    "    return grid_search_ransac_visitors.best_estimator_, grid_search_ransac_vicost.best_estimator_\n",
    "\n",
    "best_rs_visitors, best_rs_vicost = tune_gradient_boosting(X_train, y_train_visitors, y_train_vicost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visitors Model Performance:\n",
      "  MSE: 7609133369.842461\n",
      "  R²: 0.18988539057643083\n",
      "Visit/Cost Model Performance:\n",
      "  MSE: 8.812427945560647e-09\n",
      "  R²: 0.5925510817598997\n"
     ]
    }
   ],
   "source": [
    "evaluate_models(best_rs_visitors, best_rs_vicost, X_test, y_test_visitors, y_test_vicost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
