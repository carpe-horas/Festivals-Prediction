{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rapidfuzz\n",
    "문자열 간 유사도를 바탕으로 몇 %나 같은 문자열을 지녔는지. %를 score로 변환하여 반환.  \n",
    "\n",
    "단순히 같은 문자열만 지닌 것으로 anagram 을 찾지 않도록 순서 또한 신경써서 찾아냄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process as rf_process\n",
    "from rapidfuzz import fuzz as rf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 합성할 두 데이터를 불러냄.\n",
    "df1 = pd.read_csv('../data/전국연도별방문자추이.csv', encoding='euc-kr')\n",
    "df2 = pd.read_csv('../data/회계정보전처리.csv', encoding='utf-8')\n",
    "\n",
    "index_col1 = 'index'  \n",
    "index_col2 = 'index'  \n",
    "\n",
    "#들어간 값을 정규화. 공백 삭제, nan 값이면 빈칸 반환. _로 구분되어있으면 \"_\", \"\"를 사용\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return str(text).lower().replace(\" \", \"\")\n",
    "\n",
    "# df1 과 df2에서 target으로 할 열에 normalize_test 함수 적용.\n",
    "df1['normalized_index'] = df1[index_col1].apply(normalize_text)\n",
    "df2['normalized_index'] = df2[index_col2].apply(normalize_text)\n",
    "\n",
    "\n",
    "# rapidfuzz에서 유사도를 측정한 socre를 기준으로 추가적인 dataset 생성. 비교할 데이터셋 두개의 정보를 모두 담음.\n",
    "# rf_process.extractOne = 가장 유사한 문자열 종류, 점수, 인덱스를 튜플로 반환. (비교해야 할 index, 비교대상, 점수 측정방식)\n",
    "\n",
    "\n",
    "similarities = []\n",
    "for idx1, original_index in zip(df1['normalized_index'], df1[index_col1]):              #zip으로 두개씩 묶여서 저장되어 있는 normalized index 열과 기본 index 열을 for문으로 하나씩 추출\n",
    "    match = rf_process.extractOne(idx1, df2['normalized_index'], scorer=rf.ratio)       #extractOne 함수를 사용해서 normalized index 열과 df2의 normalized index 열을 비교\n",
    "    similarities.append({\n",
    "        'df1_original': original_index,\n",
    "        'df1_normalized': idx1,\n",
    "        'df2_best_match': match[0],                                                     # extractOne 함수의 결과 값은 (result_string, similarity_score, index_in_choices) 이렇게 반환.\n",
    "        'df2_original': df2[df2['normalized_index'] == match[0]][index_col2].values[0], # extractOne으로 찾은 가장 높은 유사도를 가진 값과 같은 행의 'index' 열을 추출. \n",
    "        'similarity_score': match[1]\n",
    "    })\n",
    "\n",
    "similarities_df = pd.DataFrame(similarities)                                            # dict로 묶어둔 값들을 dataframe으로 전환. 각 column의 이름은 위에서 dict로 묶어둔 저 5개\n",
    "\n",
    "#similarities_df에 df1의 원래 인덱스, df2의 원래 인덱스, 전처리가 된 df1,2의 인덱스들, 유사도 점수를 모두 포함하여, 잘못된게 있나 직접 눈으로 확인하기 좋음.\n",
    "\n",
    "# 유사도 기준 필터링 (예: 80 이상)\n",
    "# 위의 데이터는 하나라도 같다면 모두 choice에 담아두기 때문에, 아래의 스코어를 통해서 일정 이상 유사도를 지닌 것들만 남김.\n",
    "filtered_df = similarities_df[similarities_df['similarity_score'] >= 80]\n",
    "\n",
    "#filtered_df.to_csv('./data/matched_similarities.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/matched_similarities.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m matched_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/matched_similarities.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m df2_matched \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[0;32m      4\u001b[0m     df1, \n\u001b[0;32m      5\u001b[0m     matched_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf1_original\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf2_original\u001b[39m\u001b[38;5;124m'\u001b[39m]], \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m  \n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     12\u001b[0m final_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[0;32m     13\u001b[0m     df2_matched,  \n\u001b[0;32m     14\u001b[0m     df2,  \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[0;32m     18\u001b[0m )\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/matched_similarities.csv'"
     ]
    }
   ],
   "source": [
    "# 여긴 두번째 시도와 같은 방식이라 두번째 시도에서 설명.\n",
    "\n",
    "matched_df = pd.read_csv('../data/matched_similarities.csv')\n",
    "\n",
    "df2_matched = pd.merge(\n",
    "    df1, \n",
    "    matched_df[['df1_original', 'df2_original']], \n",
    "    left_on='index',  \n",
    "    right_on='df1_original',  \n",
    "    how='left'  \n",
    ")\n",
    "\n",
    "\n",
    "final_df = pd.merge(\n",
    "    df2_matched,  \n",
    "    df2,  \n",
    "    left_on='df2_original',  \n",
    "    right_on='index', \n",
    "    how='left' \n",
    ")\n",
    "\n",
    "print(final_df.head())\n",
    "\n",
    "#final_df.to_csv('./data/전국연도별방문자회계정보.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 결과 나온것 전처리.\n",
    "df = df.drop(['index_y', \n",
    "              'normalized_index_y',\n",
    "              'normalized_index_x', \n",
    "              'df1_original',\n",
    "              'df2_original',\n",
    "              'index_x'],\n",
    "                axis = 1)\n",
    "df['index'] = df['축제명'] + df['개최년도'].astype(str)\n",
    "#df.to_csv('./data/전국연도별방문자회계정보진짜최종.csv', index=False, encoding='utf-8-sig')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1차 합성은 실패.\n",
    "뒤에 합쳐진 년도 2024 등 때문에 기본적인 유사도가 굉장히 올라가있는 상태라 한글 단어가 2~3개만 매칭되더라도 같은 글자라 판단해서 합쳐버림.\n",
    "\n",
    "ex) 정선 아리랑 2024, 정선 아리랑 2019, 정? 아리랑2024가 있다면 정선 아리랑 2024와 정? 아리랑 2024가 같은 글자라 판단되서 매칭됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rapidfuzz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrapidfuzz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m process \u001b[38;5;28;01mas\u001b[39;00m rf_process\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrapidfuzz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fuzz\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rapidfuzz'"
     ]
    }
   ],
   "source": [
    "from rapidfuzz import process as rf_process\n",
    "from rapidfuzz import fuzz\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('./전국연도별방문자추이.csv', encoding='euc-kr')\n",
    "df2 = pd.read_csv('./회계정보전처리.csv', encoding='utf-8')\n",
    "\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return str(text).lower().replace(\" \", \"\")\n",
    "\n",
    "df1['normalized_index'] = df1['index'].apply(normalize_text)\n",
    "df2['normalized_index'] = df2['index'].apply(normalize_text)\n",
    "\n",
    "\n",
    "#개최년도, 회계년도가 같을 경우에만 타겟 하기 위해 추가. object로 설정되어있는 년도 column 들을 다시 int로 변환.\n",
    "df1['개최년도'] = df1['개최년도'].astype(int)\n",
    "df2['회계연도'] = df2['회계연도'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "matches = []\n",
    "for _, row1 in df1.iterrows():                  # iterrows 함수는 index, row의 튜플을 반환. index는 사용하지 않을 반환 값이므로 _로 사용하지 않음 처리.\n",
    "    match = rf_process.extractOne(              # 위에서 한 매치와 동일.\n",
    "        row1['normalized_index'],\n",
    "        df2['normalized_index'],\n",
    "        scorer=fuzz.ratio\n",
    "    )\n",
    "\n",
    "\n",
    "    if match and match[1] >= 75:                # 매치 점수의 조건. 75점 이상. 비슷한 단어가 없는 지용제를 제외한다면 최소 4글자의 축제명 + 4개의 년도해서 2개 까지 보는 걸로 75점을 기준.\n",
    "        matched_row = df2[df2['normalized_index'] == match[0]]                  # 미리 EDA에서 확인한 기본 정보를 바탕으로 함. 전부 매치가 안된게 있었다면 점수를 낮췄을 것.\n",
    "        if not matched_row.empty:                                               # match 된 행이 없을 경우를 제외. 없을 경우 row 수가 부족해지게됨. == > 결측치 체크 가능.\n",
    "            matched_row = matched_row.iloc[0]                                   # noramlized 된 데이터와 일반 데이터가 같을 경우 여러 개가 매칭되기 때문에 첫번째 df 기본 셋을 타겟으로 지정.\n",
    "            if (                                                                # 추가 조건 설정 '도'와 '지역명'의 앞 2글자가 같아야 함. \n",
    "                row1['도'][:2] == matched_row['지역명'][:2] and  \n",
    "                row1['개최년도'] == matched_row['회계연도']                      # 추가 조건 설정 회계년도와 개최년도도 같아야 함.\n",
    "            ):\n",
    "                matches.append({                                                # 1차 시도에서 했던것과 동일. 복붙하려다 삭제하기 귀찮아서 단순하게 변경.\n",
    "                    'df1_index': row1['index'],\n",
    "                    'df2_index': matched_row['index'],\n",
    "                    'similarity_score': match[1],\n",
    "                    '도': row1['도'],\n",
    "                    '개최년도': row1['개최년도']\n",
    "                })\n",
    "\n",
    "matches_df = pd.DataFrame(matches)\n",
    "\n",
    "print(matches_df)\n",
    "# 위에서도 매치 정보 저장하긴했는데, 의미 있는지는 잘 모르겠지만 하는게 좋다해서 일단 해둠.\n",
    "#matches_df.to_csv('./matched_results_75.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      축제명  개최년도  축체기간(일)  (현지인)방문자수  (외지인)방문자수  (외국인)방문자수  (전체)방문자수  일평균 방문자수  \\\n",
      "0  강경젓갈축제  2018        5    50642.0    59291.0      108.0  110041.0  22008.20   \n",
      "1  강경젓갈축제  2022        5    57971.0    69910.0       19.0  127900.0  25580.00   \n",
      "2  강경젓갈축제  2023        4    52282.0    71999.0       18.0  124299.0  31074.75   \n",
      "3  강릉커피축제  2018        5    61368.0    82621.0      279.0  144268.0  28853.60   \n",
      "4  강릉커피축제  2019        4    98907.0    61246.0      743.0  160896.0  40224.00   \n",
      "\n",
      "   전년도 일평균 방문자수  일평균 방문자수 증감률  ...    회계연도  지역명   행사축제명           총비용  \\\n",
      "0           NaN           NaN  ...  2018.0   충남  강경젓갈축제  8.250000e+08   \n",
      "1       22008.2          16.2  ...  2022.0   충남  강경젓갈축제  1.003000e+09   \n",
      "2       25580.0          21.5  ...  2023.0   충남  강경젓갈축제  1.178086e+09   \n",
      "3           NaN           NaN  ...  2018.0   강원  강릉커피축제  3.904450e+08   \n",
      "4       28853.6          39.4  ...  2019.0   강원  강릉커피축제  4.400000e+08   \n",
      "\n",
      "          사업수익           순원가   시작월   종료월     index_y normalized_index_y  \n",
      "0  183950000.0  6.410500e+08  10.0  10.0  강경젓갈축제2018         강경젓갈축제2018  \n",
      "1          0.0  1.003000e+09  10.0  10.0  강경젓갈축제2022         강경젓갈축제2022  \n",
      "2          0.0  1.178086e+09  10.0  10.0  강경젓갈축제2023         강경젓갈축제2023  \n",
      "3   40000000.0  3.504450e+08  10.0  10.0  강릉커피축제2018         강릉커피축제2018  \n",
      "4   91000000.0  3.490000e+08  10.0  10.0  강릉커피축제2019         강릉커피축제2019  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "matched_df = matches_df.copy()                  # 데이터 건드리면 안되니까 카피 생성해서 사용.\n",
    "\n",
    "#df1에 matched_df사용해서 df1의 인덱스에 df2의 인덱스를 매핑해서 머지.\n",
    "df2_matched = pd.merge(\n",
    "    df1, \n",
    "    matched_df[['df1_index', 'df2_index']], \n",
    "    left_on='index',  \n",
    "    right_on='df1_index',  \n",
    "    how='left'  \n",
    ")\n",
    "\n",
    "#df1에 들어간 matched_df의 df2 대상 index 정보를 바탕으로 df2를 매핑해서 머지함.\n",
    "final_df = pd.merge(\n",
    "    df2_matched,  \n",
    "    df2,  \n",
    "    left_on='df2_index',  \n",
    "    right_on='index', \n",
    "    how='left' \n",
    ")\n",
    "\n",
    "print(final_df.head())\n",
    "\n",
    "#완성~ 드랍은 수연님이 해주신 걸로 데이터셋 생성 완료.\n",
    "#final_df.to_csv('./전국연도별방문자회계정보정망정말최종ver.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
